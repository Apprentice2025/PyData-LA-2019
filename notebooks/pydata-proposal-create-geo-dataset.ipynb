{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Experimental ML with Holoviews/Geoviews + PyTorch in Jupyterlab\n",
    "\n",
    "- type: PyData LA 2019 Proposal\n",
    "- date: 2019-09-21\n",
    "- author: Hayley Song (haejinso@usc.edu)\n",
    "\n",
    "- Prereq: \n",
    "    - Basic understanding of visaulization with python (eg. previously have used matplotlib.pyplot library)\n",
    "    - Basic understanding of neural network training process   \n",
    "    I'll give a brief overview of the workflow, assuming audiences' previous experience with the following concepts\n",
    "        - mini-batch training\n",
    "        - forward-pass, backword-pass \n",
    "        - gradient, gradient descent algorithm\n",
    "        - classification, semantic segmentation\n",
    "        - image as numpy ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Overview + Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This talk introduces explorative and interactive tools you can incorporate into your modeling workflow using the combination of JupyterLab, Holoviews, and PyTorch.   I will demonstrate step-by-step guides to show how Holoviews/Geoviews can help explore your dataset and monitor the training process more interactively.  In addition, I will show how the `param` library facilitates interactive (hyper)parameter tuning when we define a nerual network as a subclass of `Param.Parametrized`. Compared to conventional ways to specify  hyperparameter settings (eg. via 'argparse' library or config files), this way of defining a model allows you to change and experiement the hyperparameters within the same session of your Jupyter notebook with a graphical user interface (GUI).  To illustrate the steps, I will focus on the problem of classifying different road types from satellite images as a multi-class semantic segmentation problem.  Starting from the data exploration to the trained model inspection, the audiences will learn different ways to interact with their data, the hyperparameter space of their models, and the trained model's outputs, all in the same notebook session. \n",
    "\n",
    "In summary, by the end of the talk the audience will have :\n",
    "- understood the benefits of using Holoviews/Geoviews for interactive data exploration and training monitoring \n",
    "- understood how to explore different hyperparameters for their a neural network model using the `Param` library \n",
    "- had template-like examples to easily apply to their own datasets and neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![Geoviews-RGB-Mask](../assets/hv-rgb-and-mask.png)\n",
    "\n",
    "Figure 1: Using Geoviews to visualize RGB and Mask images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "\n",
    "![](../assets/gui-osm-downloader.png)\n",
    "\n",
    "Figure 2: Using `panel`, `param` to create a GUI to link external OSM data   \n",
    "        Left: Satellite RGB and Segmentation Mask. Right: OSM roadlines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "My talk will consists of five main components. The flow is in alignment with the general modeling procedure.\n",
    "- Step 1: Explore your dataset with `Holoviews`/`Geoviews`\n",
    "- Step 2: Build an easily-configurable neural network model with `param` \n",
    "- Step 3: Monitor your training process \n",
    "- Step 4: Analyze your learned model on new images\n",
    "- Step 5: Understand what your model has learned by looking at intermediate feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "pyViz is --- .  \n",
    "PyTorch is --- . \n",
    "\n",
    "\n",
    "The general modeling workflow in the context of training a neural network is:\n",
    "1. Prepare your dataset\n",
    "2. Train your model with train and validation datasets\n",
    "3. Test your trained model on the test dataset\n",
    "    - Understand what your model has learned by inspecting the intermediate layers: \"feature maps\"\n",
    "\n",
    "    \n",
    "Throughout this talk, I will show how we can use pyViz tools to add interactive explorations into this workflow without much difficulties. These tools make it more intuitive and easy to explore your datasets, control the training process and inspect what is happening at each step, and understand the test outputs and the learned model in a deeper way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 1: Explore your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- Prepare your dataset: train, validation, test\n",
    "    - classification: \n",
    "        - eg: airplane/not-airplane, cat/dog/giraffe, land cover classifiation (forest, road, ...)\n",
    "        - eg: semantic segmentation: classify each pixel into a label in the label categories\n",
    "    - clustering: \n",
    "    \n",
    "This talk focuses on the semantic segmentation. So our dataset consists of the input image (RGB) and the target image which will be a \"mask\" image whose pixel has one of the labels in {'highway', 'track', 'dirt', 'others'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 2: Monitor the training process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 3: Interactively test your trained model on the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 4: What have the model learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 5: Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Step 6: Summary\n",
    "- Main Takeway\n",
    "\n",
    "- Resources\n",
    "    - General: \n",
    "        - Github repo for this talk:\n",
    "        - PyViz libraries:\n",
    "            - Holoviews, Geoviews, Panel, Param\n",
    "            - scipy talk\n",
    "            - more: DataShader\n",
    "            \n",
    "        - PyTorch:\n",
    "            - torchvision\n",
    "\n",
    "    - Data:\n",
    "        - Remote sensing data: google-earth-engine\n",
    "        - xarray, dash, \n",
    "        - Spatial data: rasterio, geopandas (,xarray)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earthml_v2]",
   "language": "python",
   "name": "conda-env-earthml_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
